{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import OPTICS\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 주어진 데이터 프레임 생성\n",
        "data = {\n",
        "    'PC1': [4.268221, 43.018170, 12.690767, -11.206257, -12.890127, -6.877754, 11.830796, -13.748025, -5.242339, 6.089911],\n",
        "    'PC2': [-51.277805, -5.458323, -1.213626, -12.641719, -0.349350, -2.149095, -0.143538, -8.015132, 5.318019, -3.421382],\n",
        "    'PC3': [-10.480102, 4.651520, -1.368500, 8.621291, -0.633938, -0.108776, 0.335296, 15.943814, -8.142536, 1.917324],\n",
        "    'PC4': [-8.524084, 3.484968, 0.835437, 23.878764, 12.915047, 8.001788, 0.264470, 5.850048, -4.727296, 3.624886],\n",
        "    'PC5': [0.649149, 0.952031, -0.575149, -7.124372, 4.973803, 0.706754, 0.242483, 1.744722, -3.060403, 0.457345]\n",
        "}\n",
        "\n",
        "X = pd.DataFrame(data)\n",
        "\n",
        "# 두 개의 차원 선택 (예: PC1, PC2)\n",
        "selected_dimensions = ['PC1', 'PC2']\n",
        "X_selected = X[selected_dimensions]\n",
        "\n",
        "# Function to perform OPTICS clustering and visualize the results\n",
        "def OPTICS_cluster(X, eps, min_samples):\n",
        "\n",
        "    # Initialize the OPTICS clustering algorithm with eps and min_samples.\n",
        "    optics = OPTICS(eps=eps, min_samples=min_samples)\n",
        "\n",
        "    # Fit the OPTICS model to the data and obtain cluster labels.\n",
        "    cluster_labels = optics.fit_predict(X)\n",
        "\n",
        "    # Get unique cluster labels (excluding noise points with label -1)\n",
        "    unique_labels = set(cluster_labels) - {-1}\n",
        "\n",
        "    # Plot the cluster results for the selected dimensions on a single XY plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "    # Assign colors to clusters\n",
        "    colors = plt.cm.get_cmap('tab20', len(unique_labels))\n",
        "\n",
        "    for k, col in zip(unique_labels, colors(range(len(unique_labels)))):\n",
        "        class_member_mask = (cluster_labels == k)\n",
        "        xy = X[class_member_mask]\n",
        "        ax.plot(xy.iloc[:, 0], xy.iloc[:, 1], \"o\", markerfacecolor=col, markeredgecolor=\"k\", markersize=6, label=f'Cluster {k}')\n",
        "\n",
        "    # Plot noise points with light gray color\n",
        "    noise_points = X[cluster_labels == -1]\n",
        "    ax.plot(noise_points.iloc[:, 0], noise_points.iloc[:, 1], \"o\", markerfacecolor=\"lightgray\", markeredgecolor=\"k\", markersize=6, label='Noise')\n",
        "\n",
        "    ax.set_title(f\"OPTICS Cluster Results (eps={eps}, min_samples={min_samples})\")\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Get the number of clusters formed.\n",
        "    num_clusters = len(unique_labels)\n",
        "    print(f\"For the epsilon value of {eps} and min_samples of {min_samples}, the number of clusters is {num_clusters}\")\n",
        "\n",
        "    return cluster_labels\n",
        "\n",
        "# Function to optimize and get the optimal eps value\n",
        "def optimise_epsilon(X):\n",
        "\n",
        "    # Initialize the NearestNeighbors function with 2 neighbors\n",
        "    # This is to find the optimal value of epsilon based on an optimization algorithm\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    neigh = NearestNeighbors(n_neighbors=2)\n",
        "\n",
        "    # Fits the model with X\n",
        "    nbrs = neigh.fit(X)\n",
        "\n",
        "    # Using the method .kneighbors(X), the k-Neighbours for data points in the dataset X are found\n",
        "    # By default, this method returns the distances array which is found in the 0 index\n",
        "    distances = nbrs.kneighbors(X)[0]\n",
        "\n",
        "    # Sort distances row-wise\n",
        "    distances = np.sort(distances, axis=0)\n",
        "\n",
        "    # Remove the 0-index value (Euclidean distance to itself which is 0)\n",
        "    distances = distances[:,1:]\n",
        "\n",
        "    # Plot graph of \"Euclidean Distance\" against \"Datapoint\".\n",
        "    # Using this graph, the optimal epsilon value can be found via the elbow method\n",
        "    plt.plot(distances, color=\"r\")\n",
        "    plt.title(\"Sorted Euclidean Distance to Nearest Neighbor\")\n",
        "    plt.xlabel(\"Data Point\")\n",
        "    plt.ylabel(\"Euclidean Distance\")\n",
        "    plt.axhline(y=12.5, color='black', linestyle='--')  # Adjust this threshold as needed\n",
        "    plt.show()\n",
        "\n",
        "    # Use the elbow method or other criteria to determine the optimal epsilon value\n",
        "    # In this example, we set the threshold manually (you can adjust it as needed)\n",
        "    optimal_eps = 12.5\n",
        "\n",
        "    return optimal_eps\n",
        "\n",
        "# Find the optimal epsilon value\n",
        "optimal_eps = optimise_epsilon(X_selected)\n",
        "\n",
        "# Use the optimal epsilon value in the OPTICS clustering for the selected dimensions\n",
        "OPTICS_cluster(X_selected, eps=optimal_eps, min_samples=5)\n"
      ],
      "metadata": {
        "id": "PMiZmlKK-a5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MyKtwgnZMiwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2oJUMLjGj4lb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}