{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1vqTeb9v8VB20T8z5ZeqHfHrsEv6nyhRg",
      "authorship_tag": "ABX9TyO06ncMdoP33jjETq8gHnfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moonstar0301/PairsTrading_PaperReadingGroup/blob/main/%EC%BD%94%EC%8A%A4%ED%94%BC%20%EC%A0%84%EC%A2%85%EB%AA%A9%EC%97%90%EC%84%9C%EC%9D%98%20%ED%8E%98%EC%96%B4%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%94%A9%20%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/sources/data_paristrading.xlsx\", index_col=0)\n",
        "\n",
        "df.index.name = 'Date'\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "1mvN4Xp2aWlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:2]"
      ],
      "metadata": {
        "id": "pL3fTxrugLPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy3KuRKgR3zv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import math\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "\n",
        "'''Import data'''\n",
        "\n",
        "class Constants: # object to store constants we need\n",
        "    def __init__(self, start_train, end_train, start_test, end_test):\n",
        "        self.start_train = start_train\n",
        "        self.end_train = end_train\n",
        "        self.start_test = start_test\n",
        "        self.end_test = end_test\n",
        "\n",
        "def normalize_data(df, column_name):\n",
        "    df[column_name] = (df[column_name] - df[column_name].mean()) / df[column_name].std()\n",
        "\n",
        "def get_paths(fpath, extension=\"\"):\n",
        "    '''\n",
        "    returns dictionary of {stock_name:path_to_csv}\n",
        "    '''\n",
        "    all_files = glob.glob(fpath + f\"/*{extension}\") # get all paths in directory\n",
        "    return {os.path.basename(v):v for v in all_files}\n",
        "\n",
        "def dt_parse(s):\n",
        "    return dt.datetime.strptime(s, \"%Y-%m-%d\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dateutil.parser import parse as dt_parse  # dt_parse 함수를 가져옵니다.\n",
        "\n",
        "def get_dataframes(constants):\n",
        "    start_train = dt_parse(constants.start_train)\n",
        "    end_train = dt_parse(constants.end_train)\n",
        "    end_test = dt_parse(constants.end_test)\n",
        "\n",
        "    df = pd.read_excel(\"/content/drive/MyDrive/sources/data_paristrading.xlsx\", index_col=0)\n",
        "    df.index.name = 'Date'\n",
        "\n",
        "    df_list = []  # 로그 수익률을 저장할 리스트\n",
        "\n",
        "    for symbol in df.columns:\n",
        "        if df.index[0] >= start_train or df.index[-1] <= end_test:\n",
        "            # 주어진 기간에 해당하지 않는 데이터는 건너뜁니다.\n",
        "            continue\n",
        "\n",
        "        log_returns = np.log(df[symbol] / df[symbol].shift(1))  # 로그 수익률 계산\n",
        "        log_returns = log_returns[start_train:end_train].dropna()  # 훈련 기간 데이터 추출 및 NaN 값 제거\n",
        "        log_returns.name = symbol  # 컬럼 이름 설정\n",
        "        df_list.append(log_returns)\n",
        "\n",
        "    return df_list\n",
        "\n",
        "# constants 객체를 만들어서 함수에 전달합니다.\n",
        "constants = Constants(\"2016-07-01\", \"2021-07-01\", \"2021-07-02\", \"2023-07-02\")\n",
        "\n",
        "df_list = get_dataframes(constants)\n",
        "# dataframes 리스트에는 각 주식의 로그 수익률이 들어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat = pd.concat(df_list,axis=1)\n",
        "cat = cat.dropna(axis=1, thresh=int(len(cat)*0.75)) # we require that at least 75% of the data is present\n",
        "cat = cat.bfill(axis=1) # fill nans by backfilling\n",
        "cat_T = cat.transpose()\n",
        "cat_T = cat_T.dropna(axis=1)\n",
        "\n",
        "print(\"Concatenated dataframe along columns:\\n\", cat.iloc[:5,:5])\n",
        "print(\"\\nTaking the transpose:\\n\", cat_T.iloc[:5,:5])\n",
        "print(\"\\nDimension of cat_T:\", cat_T.shape)"
      ],
      "metadata": {
        "id": "6NvWEkLxSexU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "cat_T.values returns a np.array of log returns\n",
        "We will standardise these log returns using StandardScaler\n",
        "'''\n",
        "\n",
        "norm = StandardScaler().fit_transform(cat_T.values)\n",
        "norm_symbols = cat_T.index\n",
        "\n",
        "print(\"Values before standardisation:\\n\", cat_T.values[:3])\n",
        "print(\"\\nValues after standardisation:\\n\", norm[:3])\n",
        "print(np.mean(norm).round(1), np.var(norm).round(1))"
      ],
      "metadata": {
        "id": "Ua0b9opqShPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA"
      ],
      "metadata": {
        "id": "XrlIlWTqSjtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Dimensionality Reduction with PCA before we apply any clustering algos:\n",
        "'''\n",
        "\n",
        "def optimise_pca(norm):\n",
        "    explained_var = []\n",
        "    principal_components = []\n",
        "    for i in range(0,733,1): # For loop from 0 to 49 principal components to calculate their respective percentage of variance.\n",
        "\n",
        "        # Initialise the PCA function with the i number of principal components.\n",
        "        pca_model = PCA(n_components=i)\n",
        "\n",
        "        # Fits the model with norm and applies PCA on norm to return dataset with i number of principal components\n",
        "        pca_model.fit_transform(norm)\n",
        "\n",
        "        # Using the attribute \".explained_variance_ratio\" returns percentage of variance explained by each selected component.\n",
        "        # This result is then summed up to obtained percentage of variance explained for i number of components.\n",
        "        # It is appended into explained_var to be plotted as the y-axis.\n",
        "        explained_var.append(sum(pca_model.explained_variance_ratio_))\n",
        "\n",
        "        # The number of principal components, i, is appended to principal_components to be plotted as the x-axis.\n",
        "        principal_components.append(i)\n",
        "\n",
        "    # Plot graph of \"Percentage of variance explained\" against \"Number of principal components\"\n",
        "    # Using this graph, the optimal number of dimensions can be found via the elbow method\n",
        "    plt.plot(principal_components, explained_var, color=\"red\", marker=\"o\")\n",
        "    plt.title(\"Graph of variance explained against no. of PCs\")\n",
        "    plt.xlabel(\"Number of principal components\")\n",
        "    plt.ylabel(\"Percentage of variance explained\")\n",
        "    plt.axvline(270, color='black', linestyle='--')\n",
        "    plt.show()\n",
        "\n",
        "def pca(norm, symbols, n):\n",
        "\n",
        "    # Initialise the PCA function with the n number of principal components.\n",
        "    pca_model = PCA(n_components=n)\n",
        "\n",
        "    # Fits the model with norm and applies PCA on norm to return dataset with n number of principal components\n",
        "    components = pca_model.fit_transform(norm)\n",
        "\n",
        "    # print(f\"{n} largest eigenvalues of the covariance matrix:\\n{pca_model.explained_variance_}\")\n",
        "    # print(f\"Percentage of variance explained by each component:\\n{pca_model.explained_variance_ratio_}\")\n",
        "    print(f\"Total percentage variance explained with {n} principal components: {round(sum(pca_model.explained_variance_ratio_),3)}\\n\")\n",
        "    print()\n",
        "\n",
        "    # Create dataset (that has been reduced to n number of principal components) in proper format using pd.Dataframe(data,columns,index):\n",
        "    pca_df = pd.DataFrame(components, columns=[f\"PC{k}\" for k in range(1,n+1)],index=symbols)\n",
        "    return pca_df\n",
        "\n",
        "optimise_pca(norm)"
      ],
      "metadata": {
        "id": "LUoIH6dCSlNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_df = pca(norm, norm_symbols, n=270)\n",
        "print(pca_df.iloc[:,:5])"
      ],
      "metadata": {
        "id": "6EQ9wU0py7PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Make a pointer to store clustering results'''\n",
        "\n",
        "cluster_results = {}\n",
        "\n",
        "'''K-Means Clustering'''\n",
        "\n",
        "def optimise_k(pca_df):\n",
        "\n",
        "    error = []\n",
        "    ks = []\n",
        "\n",
        "    for i in range(10, 300, 20): # For loop from 10 to 200 with intervals of 5 and calculate the respective sum of squared distances for i number of centroids\n",
        "\n",
        "        # Initialise the KMeans function with the i number of centroids.\n",
        "        # By default, KMeans function uses the KMeans++ Method of initialisation the centroids.\n",
        "        kmeans = KMeans(n_clusters=i)\n",
        "\n",
        "        # Fits the model with pcd_df and applies KMeans algorithm on pca_df.\n",
        "        kmeans.fit(pca_df)\n",
        "\n",
        "        # Using the attribute \".inertia_\" returns the sum of squared distances of datapoints to their closest cluster center.\n",
        "        # It is appended into error to be plotted as the y-axis.\n",
        "        error.append(kmeans.inertia_)\n",
        "\n",
        "        # The number of centroids, i, is appended to ks to be plotted as the x-axis.\n",
        "        ks.append(i)\n",
        "\n",
        "    # Plot graph of \"Sum of Squared Distances\" against \"k value\".\n",
        "    # Using this graph, the optimal number of centroids can be found via the elbow method\n",
        "    plt.plot(ks, error, marker=\"o\", color=\"r\")\n",
        "    plt.title(\"Graph of sum of squared distance against k\")\n",
        "    plt.xlabel(\"k value\")\n",
        "    plt.ylabel(\"Sum of Squared Distances\")\n",
        "    plt.axvline(150, color='black', linestyle='--')\n",
        "    plt.show()\n",
        "\n",
        "def cluster_kmeans(pca_df, k):\n",
        "\n",
        "    # Initialise the KMeans function with the k number of centroids.\n",
        "    kmeans = KMeans(n_clusters=k)\n",
        "\n",
        "    # Fits the model with pcd_df and applies KMeans algorithm on pca_df.\n",
        "    kmeans.fit(pca_df)\n",
        "\n",
        "    # Using the method .predict(X), it calculates the closest cluster each datapoint in dataset X belongs to.\n",
        "    results = kmeans.predict(pca_df)\n",
        "\n",
        "    # Plot the cluster results.\n",
        "    fig, ax = plt.subplots(figsize=(9,6))\n",
        "    pca_df.plot(x=\"PC1\", y=\"PC2\", kind=\"scatter\",\n",
        "        title=f\"K-Means Cluster Results, k={k}\", c=results, cmap=\"jet\", ax=ax)\n",
        "    plt.show()\n",
        "    return results\n",
        "\n",
        "optimise_k(pca_df)\n",
        "print(f\"From the graph above, we can see that the optimal k-value is at the elbow, at k = 30\")"
      ],
      "metadata": {
        "id": "aJY4wPcVUkWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Plotting the K-Means Cluster Results'''\n",
        "\n",
        "# K-Means Clustering is applied on pca_df for k = 3\n",
        "kmeans_results = cluster_kmeans(pca_df,k=150)\n",
        "\n",
        "# K-Means Clustering results are saved in cluster_results\n",
        "cluster_results[f\"KMeans\"] = kmeans_results\n",
        "\n",
        "print(f\"Therefore, we plot the K-Means results for the optimal k-value, 150\")"
      ],
      "metadata": {
        "id": "35TMEIZNU8mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN"
      ],
      "metadata": {
        "id": "swl8uwB3VF-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "DBSCAN Clustering:\n",
        "'''\n",
        "\n",
        "def optimise_epsilon(pca_df):\n",
        "\n",
        "    # Initialise the NearestNeighbors function with 2 neighbors\n",
        "    # This is to find optimal value of epsilon based on an optimisation algorithm\n",
        "    neigh = NearestNeighbors(n_neighbors=2)\n",
        "\n",
        "    # Fits the model with pcd_df\n",
        "    nbrs = neigh.fit(pca_df)\n",
        "\n",
        "    # Using the method .kneighbors(X), the k-Neighbours for datatpoints in datatset X are found\n",
        "    # By default, this method returns the distances array which is found in the 0 index\n",
        "    distances = nbrs.kneighbors(pca_df)[0]\n",
        "\n",
        "    # Sort distances row-wise\n",
        "    distances = np.sort(distances,axis=0)\n",
        "\n",
        "    # Remove the 0-index value (Euclidean distance to itself which is 0)\n",
        "    distances = distances[:,1:]\n",
        "\n",
        "    # Plot graph of \"Euclidean Distance\" against \"Datapoint\".\n",
        "    # Using this graph, the optimal epsilon value can be found via the elbow method\n",
        "    plt.plot(distances, color=\"r\")\n",
        "    plt.title(\"Sorted euclidean distance to nearest neighbour\")\n",
        "    plt.xlabel(\"Datapoint\")\n",
        "    plt.ylabel(\"Euclidean Distance\")\n",
        "    plt.axhline(y=50, color='black', linestyle='--')\n",
        "    plt.show()\n",
        "\n",
        "def cluster_dbscan(pca_df, eps, min_samples):\n",
        "\n",
        "    # Initialise the DBSCAN function with eps and min_samples.\n",
        "    # eps is a float value and is the maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
        "    # min_samples is the number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
        "    # This includes the point itself.\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "\n",
        "    # Using the method .fit_predict(X), clusters from a data or distance matrix and predict labels are computed\n",
        "    results = dbscan.fit_predict(pca_df)\n",
        "\n",
        "    # Plot the cluster results.\n",
        "    fig, ax = plt.subplots(figsize=(9,6))\n",
        "    pca_df.plot(x=\"PC1\", y=\"PC2\", kind=\"scatter\",\n",
        "        title=f\"DBSCAN Cluster Results\", c=results, cmap=\"jet\", ax=ax)\n",
        "    plt.show()\n",
        "\n",
        "    # Get number of clusters formed.\n",
        "    num_clusters = np.max(results)\n",
        "    print(f\"For the epsilon value of {eps}, the number of clusters is {num_clusters}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "optimise_epsilon(pca_df)\n",
        "print(f\"From the graph above, we can see that the optimal epsilon is at the elbow, at epsilon = 16\")"
      ],
      "metadata": {
        "id": "zQy2VumUVFfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Plotting the DBSCAN Cluster Results'''\n",
        "\n",
        "# DBSCAN Clustering is applied on pca_df for eps = 50 and min_samples = 2\n",
        "results = cluster_dbscan(pca_df, eps=50, min_samples=2)\n",
        "\n",
        "# DBSCANS Clustering results are saved in cluster_results, a dict\n",
        "cluster_results[\"DBSCAN\"] = results"
      ],
      "metadata": {
        "id": "iAwOIZRzVYjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hierarchical Clustering"
      ],
      "metadata": {
        "id": "J06eKhZAViQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Hierarchical Clustering (AgglomerativeClustering)\n",
        "'''\n",
        "\n",
        "def cluster_hierar(pca_df, n):\n",
        "\n",
        "    # Initialise the AgglomerativeClustering function with n clusters.\n",
        "    # Fits the model with pca_df and applies Agglomerative Clustering algorithm on pca_df.\n",
        "    hierar = AgglomerativeClustering(n_clusters=n, ).fit(pca_df)\n",
        "\n",
        "    # Using the method .fit_predict(X), it fits and return the result of each datapoint's clustering assignment.\n",
        "    results = hierar.fit_predict(pca_df)\n",
        "\n",
        "    # Plot cluster results.\n",
        "    fig, ax = plt.subplots(figsize=(9,6))\n",
        "    pca_df.plot(x=\"PC1\", y=\"PC2\", kind=\"scatter\",\n",
        "        title=f\"Agglomerative Clustering Results, n={n}\", c=results, cmap=\"jet\", ax=ax)\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "def draw_dendrogram(pca_df):\n",
        "\n",
        "    # Initialise Linkage function with linkage method = \"ward\".\n",
        "    # linkage returns a linkage matrix, which is needed for the dendrogram to be plotted.\n",
        "    linked = linkage(pca_df,method = \"ward\")\n",
        "\n",
        "    # Plot dendrogram results.\n",
        "    plt.figure(figsize =(10,7))\n",
        "    plt.title(\"Dendrogram\")\n",
        "    dendrogram(linked,\n",
        "                orientation = \"top\",\n",
        "                distance_sort = \"descreasing\",\n",
        "                show_leaf_counts = True)\n",
        "    plt.show()\n",
        "\n",
        "draw_dendrogram(pca_df)"
      ],
      "metadata": {
        "id": "Az7DM6O6VhHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Plotting the Hierarchical Cluster Results'''\n",
        "# Hierarchical Clustering is applied on pca_df for n_clusters = sqrt(N)\n",
        "results = cluster_hierar(pca_df, int(pca_df.shape[0]**0.5))\n",
        "\n",
        "# Hierarchical Clustering results are saved in cluster_results\n",
        "cluster_results[\"Hierarchical\"] = results\n",
        "\n",
        "print(f\"In the case of Hierarchical Clustering, we chose the optimal number of clusters to be sqrt(N)=27, where N is the number of data\")"
      ],
      "metadata": {
        "id": "_zssioWZVntS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Cluster Results -> Groupings of stocks/ETFs:\n",
        "1. First, we need to map the cluster ids to the symbols of our stocks to get groups of stocks\n",
        "2. Then store the clusters into a pointer -> \"groups\"\n",
        "'''\n",
        "\n",
        "# Return array of the symbols of stocks.\n",
        "print(norm_symbols.shape)\n",
        "\n",
        "# Contains cluster group assignments for each algorithm.\n",
        "print([x.shape for x in cluster_results.values()])\n",
        "\n",
        "def map_clusters(cluster_result):\n",
        "    grouping = [[] for _ in range(max(cluster_result)+1)]\n",
        "    for i, res in enumerate(cluster_result):\n",
        "        grouping[res].append(norm_symbols[i])\n",
        "\n",
        "    grouping = [g for g in grouping if 1<len(g)<=10] # Keep clusters with <=10 members.\n",
        "\n",
        "    return grouping\n",
        "\n",
        "groups = {}\n",
        "for algorithm, cluster_result in cluster_results.items():\n",
        "    groups[algorithm] = map_clusters(cluster_result)\n",
        "\n",
        "\n",
        "print(\"Number of valid clusters:\", {k:len(v) for k,v in groups.items()})\n",
        "#print(\"Example cluster:\", )"
      ],
      "metadata": {
        "id": "gUPDRay_V005"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groups['DBSCAN']"
      ],
      "metadata": {
        "id": "7kPbIz31tkbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groups['KMeans']"
      ],
      "metadata": {
        "id": "FmPlDgAZtsCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = pd.DataFrame(groups['KMeans'])\n",
        "k = k.T\n",
        "k.to_excel('코스피_전체_Kmeans.xlsx')"
      ],
      "metadata": {
        "id": "_4u_STKCEBdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groups['Hierarchical']"
      ],
      "metadata": {
        "id": "rk5_l3zptvSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = pd.DataFrame(groups['Hierarchical'])\n",
        "h = h.T\n",
        "h.to_excel('코스피_전체_Hierarchical.xlsx')"
      ],
      "metadata": {
        "id": "txJMi6DbEqQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Evaluating cluster results by measuring pairwise correlation within each cluster in the validation period\n",
        "'''\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/sources/data_paristrading.xlsx\", index_col=0)\n",
        "df.index.name = 'Date'\n",
        "def get_r_squared(df, group, start_test, end_test):\n",
        "    start_test = dt.datetime.strptime(start_test, \"%Y-%m-%d\")\n",
        "    end_test = dt.datetime.strptime(end_test, \"%Y-%m-%d\")\n",
        "    dfs = []\n",
        "\n",
        "\n",
        "    for symbol in df.columns:\n",
        "        if symbol in group:\n",
        "            if df.index[0] >= start_test or df.index[-1] <= end_test:\n",
        "                # 주어진 기간에 해당하지 않는 데이터는 건너뜁니다.\n",
        "                continue\n",
        "\n",
        "            log_returns = np.log(df[symbol] / df[symbol].shift(1))  # 로그 수익률 계산\n",
        "            log_returns = log_returns[start_test:end_test].dropna()  # 훈련 기간 데이터 추출 및 NaN 값 제거\n",
        "            log_returns.name = symbol  # 컬럼 이름 설정\n",
        "\n",
        "            dfs.append(df[symbol])\n",
        "\n",
        "\n",
        "    df = pd.concat(dfs, axis=1)\n",
        "    df = df[start_test:end_test] # Filter to validation period\n",
        "    corr = df.corr() # Correlation matrix\n",
        "    corr.values[np.tril_indices_from(corr)] = np.nan # Keep only upper triangular of the correlation matrix\n",
        "    return corr.unstack().mean() # Return mean of the correlation matrix\n",
        "\n",
        "def average_correlation(groups, constants):\n",
        "    avg_corr = {}\n",
        "    for algorithm, cluster_result in groups.items():\n",
        "        corrs = []\n",
        "        for group in cluster_result:\n",
        "            corr = get_r_squared(df, group, constants.start_test, constants.end_test)\n",
        "            corrs.append(corr)\n",
        "        avg_corr[algorithm] = np.mean(np.square(corrs)) # Take mean of r-squared\n",
        "    return avg_corr\n",
        "\n",
        "corr_metrics = average_correlation(groups, constants)\n",
        "pd.DataFrame(corr_metrics.items(), columns=[\"Algorithm\", \"R-Squared\"])"
      ],
      "metadata": {
        "id": "D_WRj1jvV2Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"/content/drive/MyDrive/sources/data_paristrading.xlsx\", index_col=0)\n",
        "df.index.name = 'Date'"
      ],
      "metadata": {
        "id": "JVeyj6-3-B8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 나눔글꼴 Regular으로 폰트 지정\n",
        "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "font_prop = fm.FontProperties(fname=font_path, size=20)\n",
        "# 폰트 설정\n",
        "plt.rc('font', family='NanumGothic')\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False # 마이너스 기호(-)가 깨지는 현상 방지"
      ],
      "metadata": {
        "id": "tl-3gTuaC0j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Plot the stock prices of stocks from each cluster in the validation period\n",
        "'''\n",
        "\n",
        "\n",
        "def plot_time_series(df, group, i, start_test, end_test):\n",
        "    start_test = dt.datetime.strptime(start_test, \"%Y-%m-%d\")\n",
        "    end_test = dt.datetime.strptime(end_test, \"%Y-%m-%d\")\n",
        "\n",
        "    for symbol in df.columns:\n",
        "        if symbol in group:\n",
        "            if df.index[0] >= start_test or df.index[-1] <= end_test:\n",
        "                # 주어진 기간에 해당하지 않는 데이터는 건너뜁니다.\n",
        "                continue\n",
        "\n",
        "            log_returns = np.log(df[symbol] / df[symbol].shift(1))  # 로그 수익률 계산\n",
        "            log_returns = log_returns[start_test:end_test].dropna()  # 훈련 기간 데이터 추출 및 NaN 값 제거\n",
        "            log_returns.name = symbol  # 컬럼 이름 설정\n",
        "            df[\"Cumulative Log Return\"] = log_returns.cumsum()\n",
        "            ax[i].plot(df[\"Cumulative Log Return\"][start_test:end_test]) # Plot validation period\n",
        "            ax[i].set_xlabel(\"Datetime\")\n",
        "            ax[i].set_ylabel(\"Return\")\n",
        "    ax[i].legend(group)\n",
        "\n",
        "constants = Constants(\"2016-07-01\", \"2021-07-01\", \"2021-07-02\", \"2023-07-02\")\n",
        "# Plot out clusters.\n",
        "group = groups[\"KMeans\"]\n",
        "num_clusters = len(group)\n",
        "\n",
        "fig, ax = plt.subplots(math.ceil(num_clusters/2),2, figsize = (15,20))\n",
        "ax = ax.ravel()\n",
        "for i in range(0, num_clusters):\n",
        "    plot_time_series(df, group[i], i, constants.start_test, constants.end_test)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I0VThTeJV5RV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}